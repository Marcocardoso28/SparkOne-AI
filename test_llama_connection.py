#!/usr/bin/env python3
"""
Script de teste para validar conexÃ£o com LLM local (LLAMA via Ollama)
"""

import asyncio
import json
import sys
from pathlib import Path

import httpx
from dotenv import load_dotenv

# Adicionar src ao path para importar mÃ³dulos do SparkOne
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.app.config import get_settings
from src.app.providers.chat import ChatProviderRouter


async def test_ollama_direct():
    """Testa conexÃ£o direta com Ollama"""
    print("ğŸ” Testando conexÃ£o direta com Ollama...")
    
    try:
        async with httpx.AsyncClient() as client:
            # Testar se Ollama estÃ¡ rodando
            response = await client.get("http://localhost:11434/api/tags", timeout=5.0)
            if response.status_code == 200:
                models = response.json()
                print("âœ… Ollama estÃ¡ rodando!")
                print(f"ğŸ“‹ Modelos disponÃ­veis: {[m['name'] for m in models.get('models', [])]}")
                return True
            else:
                print(f"âŒ Ollama respondeu com status {response.status_code}")
                return False
    except Exception as e:
        print(f"âŒ Erro ao conectar com Ollama: {e}")
        return False


async def test_ollama_generate():
    """Testa geraÃ§Ã£o de texto com Ollama"""
    print("\nğŸ§  Testando geraÃ§Ã£o de texto com Ollama...")
    
    settings = get_settings()
    model = settings.local_llm_model
    
    try:
        async with httpx.AsyncClient() as client:
            payload = {
                "model": model,
                "prompt": "OlÃ¡! VocÃª pode me responder em portuguÃªs?",
                "stream": False,
                "options": {
                    "temperature": 0.2,
                    "num_predict": 100
                }
            }
            
            response = await client.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=30.0
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"âœ… Modelo {model} respondeu:")
                print(f"ğŸ’¬ Resposta: {result.get('response', 'Sem resposta')}")
                return True
            else:
                print(f"âŒ Erro na geraÃ§Ã£o: {response.status_code}")
                print(f"ğŸ“„ Resposta: {response.text}")
                return False
                
    except Exception as e:
        print(f"âŒ Erro na geraÃ§Ã£o de texto: {e}")
        return False


async def test_sparkone_chat_provider():
    """Testa o ChatProviderRouter do SparkOne"""
    print("\nğŸš€ Testando ChatProviderRouter do SparkOne...")
    
    try:
        settings = get_settings()
        chat_router = ChatProviderRouter(settings)
        
        if not chat_router.available:
            print("âŒ Nenhum provedor de chat disponÃ­vel")
            return False
        
        print("âœ… ChatProviderRouter inicializado")
        
        # Testar geraÃ§Ã£o
        messages = [
            {"role": "system", "content": "VocÃª Ã© um assistente Ãºtil que responde em portuguÃªs."},
            {"role": "user", "content": "OlÃ¡! Como vocÃª estÃ¡?"}
        ]
        
        response = await chat_router.generate(messages)
        print(f"ğŸ’¬ Resposta do SparkOne: {response}")
        return True
        
    except Exception as e:
        print(f"âŒ Erro no ChatProviderRouter: {e}")
        return False


async def test_ingestion_flow():
    """Testa o fluxo completo de ingestÃ£o com LLM local"""
    print("\nğŸ“¥ Testando fluxo de ingestÃ£o completo...")
    
    try:
        # Simular uma requisiÃ§Ã£o para o endpoint de ingestÃ£o
        payload = {
            "content": "OlÃ¡ SparkOne! VocÃª estÃ¡ usando LLAMA?",
            "user_id": "test_user",
            "session_id": "test_session",
            "metadata": {"test": True}
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8000/ingest",
                json=payload,
                timeout=30.0
            )
            
            if response.status_code == 200:
                result = response.json()
                print("âœ… IngestÃ£o bem-sucedida!")
                print(f"ğŸ“„ Resposta: {json.dumps(result, indent=2, ensure_ascii=False)}")
                return True
            else:
                print(f"âŒ Erro na ingestÃ£o: {response.status_code}")
                print(f"ğŸ“„ Resposta: {response.text}")
                return False
                
    except Exception as e:
        print(f"âŒ Erro no teste de ingestÃ£o: {e}")
        return False


def check_environment():
    """Verifica configuraÃ§Ãµes de ambiente"""
    print("ğŸ”§ Verificando configuraÃ§Ãµes de ambiente...")
    
    load_dotenv()
    settings = get_settings()
    
    print(f"ğŸ“ LOCAL_LLM_URL: {settings.local_llm_url}")
    print(f"ğŸ¤– LOCAL_LLM_MODEL: {settings.local_llm_model}")
    print(f"ğŸ”‘ OPENAI_API_KEY: {'Configurada' if settings.openai_api_key and settings.openai_api_key != 'changeme' else 'NÃ£o configurada'}")
    print(f"ğŸ¯ EMBEDDING_PROVIDER: {settings.embedding_provider}")
    
    if not settings.local_llm_url:
        print("âš ï¸  LOCAL_LLM_URL nÃ£o estÃ¡ configurada!")
        return False
    
    return True


async def main():
    """FunÃ§Ã£o principal de teste"""
    print("ğŸ§ª Iniciando testes de conexÃ£o LLAMA com SparkOne\n")
    
    # Verificar ambiente
    if not check_environment():
        print("\nâŒ ConfiguraÃ§Ã£o de ambiente invÃ¡lida!")
        return
    
    # Testes sequenciais
    tests = [
        ("ConexÃ£o Ollama", test_ollama_direct),
        ("GeraÃ§Ã£o Ollama", test_ollama_generate),
        ("ChatProvider SparkOne", test_sparkone_chat_provider),
        ("Fluxo IngestÃ£o", test_ingestion_flow),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"ğŸ§ª Executando: {test_name}")
        print('='*50)
        
        try:
            results[test_name] = await test_func()
        except Exception as e:
            print(f"âŒ Erro inesperado em {test_name}: {e}")
            results[test_name] = False
    
    # Resumo dos resultados
    print(f"\n{'='*50}")
    print("ğŸ“Š RESUMO DOS TESTES")
    print('='*50)
    
    for test_name, success in results.items():
        status = "âœ… PASSOU" if success else "âŒ FALHOU"
        print(f"{test_name}: {status}")
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    print(f"\nğŸ¯ Resultado: {passed_tests}/{total_tests} testes passaram")
    
    if passed_tests == total_tests:
        print("ğŸ‰ Todos os testes passaram! LLAMA estÃ¡ configurado corretamente.")
    else:
        print("âš ï¸  Alguns testes falharam. Verifique a configuraÃ§Ã£o.")


if __name__ == "__main__":
    asyncio.run(main())